{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trivial Case: len(input) == len(output)\n",
    "\n",
    "https://raw.githubusercontent.com/fchollet/keras/master/examples/addition_rnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/narukawinjidtrengam/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n",
      "Vectorization...\n",
      "Training Data:\n",
      "(45000, 7, 12)\n",
      "(45000, 4, 12)\n",
      "Validation Data:\n",
      "(5000, 7, 12)\n",
      "(5000, 4, 12)\n",
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 4, 12)             1548      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4, 12)             0         \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 19s 425us/step - loss: 1.8867 - acc: 0.3217 - val_loss: 1.7812 - val_acc: 0.3412\n",
      "Q 25+397  T 422  \u001b[91m☒\u001b[0m 122 \n",
      "Q 616+395 T 1011 \u001b[91m☒\u001b[0m 1112\n",
      "Q 112+7   T 119  \u001b[91m☒\u001b[0m 22  \n",
      "Q 893+6   T 899  \u001b[91m☒\u001b[0m 121 \n",
      "Q 372+802 T 1174 \u001b[91m☒\u001b[0m 102 \n",
      "Q 43+98   T 141  \u001b[91m☒\u001b[0m 121 \n",
      "Q 2+706   T 708  \u001b[91m☒\u001b[0m 22  \n",
      "Q 75+651  T 726  \u001b[91m☒\u001b[0m 121 \n",
      "Q 569+5   T 574  \u001b[91m☒\u001b[0m 121 \n",
      "Q 144+596 T 740  \u001b[91m☒\u001b[0m 111 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 341us/step - loss: 1.7178 - acc: 0.3635 - val_loss: 1.6483 - val_acc: 0.3947\n",
      "Q 548+239 T 787  \u001b[91m☒\u001b[0m 102 \n",
      "Q 871+5   T 876  \u001b[91m☒\u001b[0m 100 \n",
      "Q 7+594   T 601  \u001b[91m☒\u001b[0m 900 \n",
      "Q 9+177   T 186  \u001b[91m☒\u001b[0m 100 \n",
      "Q 16+13   T 29   \u001b[91m☒\u001b[0m 21  \n",
      "Q 43+73   T 116  \u001b[91m☒\u001b[0m 444 \n",
      "Q 533+640 T 1173 \u001b[91m☒\u001b[0m 1433\n",
      "Q 2+182   T 184  \u001b[91m☒\u001b[0m 122 \n",
      "Q 44+653  T 697  \u001b[91m☒\u001b[0m 404 \n",
      "Q 316+1   T 317  \u001b[91m☒\u001b[0m 229 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 318us/step - loss: 1.5590 - acc: 0.4174 - val_loss: 1.4771 - val_acc: 0.4477\n",
      "Q 165+315 T 480  \u001b[91m☒\u001b[0m 574 \n",
      "Q 37+139  T 176  \u001b[91m☒\u001b[0m 334 \n",
      "Q 6+67    T 73   \u001b[91m☒\u001b[0m 77  \n",
      "Q 2+818   T 820  \u001b[91m☒\u001b[0m 121 \n",
      "Q 483+41  T 524  \u001b[91m☒\u001b[0m 444 \n",
      "Q 870+83  T 953  \u001b[91m☒\u001b[0m 801 \n",
      "Q 423+1   T 424  \u001b[91m☒\u001b[0m 334 \n",
      "Q 89+45   T 134  \u001b[91m☒\u001b[0m 104 \n",
      "Q 59+546  T 605  \u001b[91m☒\u001b[0m 554 \n",
      "Q 457+139 T 596  \u001b[91m☒\u001b[0m 554 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 340us/step - loss: 1.3912 - acc: 0.4780 - val_loss: 1.3159 - val_acc: 0.5056\n",
      "Q 246+959 T 1205 \u001b[91m☒\u001b[0m 1193\n",
      "Q 0+647   T 647  \u001b[91m☒\u001b[0m 669 \n",
      "Q 752+62  T 814  \u001b[91m☒\u001b[0m 734 \n",
      "Q 981+600 T 1581 \u001b[91m☒\u001b[0m 1611\n",
      "Q 517+39  T 556  \u001b[91m☒\u001b[0m 579 \n",
      "Q 429+32  T 461  \u001b[91m☒\u001b[0m 444 \n",
      "Q 166+864 T 1030 \u001b[91m☒\u001b[0m 1012\n",
      "Q 12+541  T 553  \u001b[91m☒\u001b[0m 574 \n",
      "Q 655+38  T 693  \u001b[91m☒\u001b[0m 664 \n",
      "Q 57+374  T 431  \u001b[91m☒\u001b[0m 442 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 318us/step - loss: 1.2656 - acc: 0.5272 - val_loss: 1.2103 - val_acc: 0.5508\n",
      "Q 993+24  T 1017 \u001b[91m☒\u001b[0m 1011\n",
      "Q 568+266 T 834  \u001b[91m☒\u001b[0m 702 \n",
      "Q 34+554  T 588  \u001b[91m☒\u001b[0m 575 \n",
      "Q 174+6   T 180  \u001b[91m☒\u001b[0m 171 \n",
      "Q 291+88  T 379  \u001b[91m☒\u001b[0m 323 \n",
      "Q 553+522 T 1075 \u001b[92m☑\u001b[0m 1075\n",
      "Q 352+248 T 600  \u001b[91m☒\u001b[0m 597 \n",
      "Q 100+750 T 850  \u001b[91m☒\u001b[0m 803 \n",
      "Q 720+46  T 766  \u001b[91m☒\u001b[0m 743 \n",
      "Q 9+734   T 743  \u001b[91m☒\u001b[0m 747 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 16s 351us/step - loss: 1.1540 - acc: 0.5724 - val_loss: 1.1052 - val_acc: 0.5910\n",
      "Q 59+280  T 339  \u001b[91m☒\u001b[0m 337 \n",
      "Q 232+169 T 401  \u001b[91m☒\u001b[0m 399 \n",
      "Q 207+588 T 795  \u001b[91m☒\u001b[0m 798 \n",
      "Q 67+489  T 556  \u001b[91m☒\u001b[0m 557 \n",
      "Q 59+771  T 830  \u001b[91m☒\u001b[0m 829 \n",
      "Q 755+947 T 1702 \u001b[91m☒\u001b[0m 1606\n",
      "Q 33+298  T 331  \u001b[91m☒\u001b[0m 337 \n",
      "Q 57+406  T 463  \u001b[91m☒\u001b[0m 458 \n",
      "Q 42+10   T 52   \u001b[91m☒\u001b[0m 46  \n",
      "Q 24+12   T 36   \u001b[92m☑\u001b[0m 36  \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 327us/step - loss: 1.0403 - acc: 0.6208 - val_loss: 0.9973 - val_acc: 0.6298\n",
      "Q 6+508   T 514  \u001b[91m☒\u001b[0m 517 \n",
      "Q 807+86  T 893  \u001b[91m☒\u001b[0m 897 \n",
      "Q 778+457 T 1235 \u001b[91m☒\u001b[0m 1244\n",
      "Q 73+7    T 80   \u001b[91m☒\u001b[0m 81  \n",
      "Q 593+8   T 601  \u001b[91m☒\u001b[0m 500 \n",
      "Q 328+165 T 493  \u001b[91m☒\u001b[0m 598 \n",
      "Q 865+207 T 1072 \u001b[91m☒\u001b[0m 100 \n",
      "Q 761+27  T 788  \u001b[91m☒\u001b[0m 781 \n",
      "Q 289+762 T 1051 \u001b[91m☒\u001b[0m 1007\n",
      "Q 616+276 T 892  \u001b[91m☒\u001b[0m 891 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 332us/step - loss: 0.9439 - acc: 0.6611 - val_loss: 0.9190 - val_acc: 0.6626\n",
      "Q 9+186   T 195  \u001b[91m☒\u001b[0m 198 \n",
      "Q 279+293 T 572  \u001b[91m☒\u001b[0m 631 \n",
      "Q 61+832  T 893  \u001b[91m☒\u001b[0m 880 \n",
      "Q 7+212   T 219  \u001b[91m☒\u001b[0m 228 \n",
      "Q 184+315 T 499  \u001b[91m☒\u001b[0m 501 \n",
      "Q 29+525  T 554  \u001b[91m☒\u001b[0m 555 \n",
      "Q 139+976 T 1115 \u001b[91m☒\u001b[0m 1117\n",
      "Q 28+944  T 972  \u001b[91m☒\u001b[0m 970 \n",
      "Q 2+636   T 638  \u001b[92m☑\u001b[0m 638 \n",
      "Q 519+38  T 557  \u001b[91m☒\u001b[0m 551 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 16s 345us/step - loss: 0.8649 - acc: 0.6912 - val_loss: 0.8307 - val_acc: 0.7066\n",
      "Q 445+7   T 452  \u001b[92m☑\u001b[0m 452 \n",
      "Q 215+8   T 223  \u001b[92m☑\u001b[0m 223 \n",
      "Q 66+684  T 750  \u001b[91m☒\u001b[0m 745 \n",
      "Q 245+40  T 285  \u001b[91m☒\u001b[0m 287 \n",
      "Q 18+453  T 471  \u001b[91m☒\u001b[0m 478 \n",
      "Q 913+95  T 1008 \u001b[91m☒\u001b[0m 1006\n",
      "Q 71+698  T 769  \u001b[91m☒\u001b[0m 760 \n",
      "Q 79+931  T 1010 \u001b[91m☒\u001b[0m 1000\n",
      "Q 58+835  T 893  \u001b[91m☒\u001b[0m 890 \n",
      "Q 507+38  T 545  \u001b[91m☒\u001b[0m 540 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 17s 370us/step - loss: 0.7929 - acc: 0.7207 - val_loss: 0.7783 - val_acc: 0.7238loss: 0.7965 - ac - ETA: 1s - loss: 0.79\n",
      "Q 746+758 T 1504 \u001b[91m☒\u001b[0m 1510\n",
      "Q 888+60  T 948  \u001b[91m☒\u001b[0m 947 \n",
      "Q 46+408  T 454  \u001b[91m☒\u001b[0m 451 \n",
      "Q 682+957 T 1639 \u001b[91m☒\u001b[0m 1630\n",
      "Q 15+798  T 813  \u001b[91m☒\u001b[0m 820 \n",
      "Q 76+6    T 82   \u001b[91m☒\u001b[0m 83  \n",
      "Q 874+17  T 891  \u001b[91m☒\u001b[0m 890 \n",
      "Q 206+794 T 1000 \u001b[91m☒\u001b[0m 1002\n",
      "Q 3+672   T 675  \u001b[91m☒\u001b[0m 670 \n",
      "Q 86+149  T 235  \u001b[91m☒\u001b[0m 237 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 17s 381us/step - loss: 0.7286 - acc: 0.7446 - val_loss: 0.7015 - val_acc: 0.7544\n",
      "Q 429+977 T 1406 \u001b[91m☒\u001b[0m 1300\n",
      "Q 482+686 T 1168 \u001b[91m☒\u001b[0m 1165\n",
      "Q 58+764  T 822  \u001b[91m☒\u001b[0m 829 \n",
      "Q 442+851 T 1293 \u001b[92m☑\u001b[0m 1293\n",
      "Q 653+616 T 1269 \u001b[91m☒\u001b[0m 1276\n",
      "Q 146+9   T 155  \u001b[91m☒\u001b[0m 154 \n",
      "Q 820+24  T 844  \u001b[91m☒\u001b[0m 843 \n",
      "Q 2+366   T 368  \u001b[91m☒\u001b[0m 369 \n",
      "Q 504+237 T 741  \u001b[91m☒\u001b[0m 739 \n",
      "Q 52+129  T 181  \u001b[91m☒\u001b[0m 179 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 16s 352us/step - loss: 0.6420 - acc: 0.7749 - val_loss: 0.5742 - val_acc: 0.7973\n",
      "Q 4+176   T 180  \u001b[92m☑\u001b[0m 180 \n",
      "Q 70+68   T 138  \u001b[91m☒\u001b[0m 137 \n",
      "Q 95+284  T 379  \u001b[91m☒\u001b[0m 370 \n",
      "Q 42+98   T 140  \u001b[91m☒\u001b[0m 147 \n",
      "Q 65+508  T 573  \u001b[91m☒\u001b[0m 575 \n",
      "Q 77+614  T 691  \u001b[92m☑\u001b[0m 691 \n",
      "Q 558+433 T 991  \u001b[91m☒\u001b[0m 990 \n",
      "Q 11+98   T 109  \u001b[91m☒\u001b[0m 107 \n",
      "Q 567+87  T 654  \u001b[92m☑\u001b[0m 654 \n",
      "Q 27+36   T 63   \u001b[91m☒\u001b[0m 52  \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 15s 324us/step - loss: 0.4770 - acc: 0.8401 - val_loss: 0.3989 - val_acc: 0.8772\n",
      "Q 23+131  T 154  \u001b[91m☒\u001b[0m 153 \n",
      "Q 441+21  T 462  \u001b[92m☑\u001b[0m 462 \n",
      "Q 4+749   T 753  \u001b[92m☑\u001b[0m 753 \n",
      "Q 502+256 T 758  \u001b[91m☒\u001b[0m 757 \n",
      "Q 62+808  T 870  \u001b[91m☒\u001b[0m 879 \n",
      "Q 370+13  T 383  \u001b[92m☑\u001b[0m 383 \n",
      "Q 865+451 T 1316 \u001b[91m☒\u001b[0m 1317\n",
      "Q 428+51  T 479  \u001b[92m☑\u001b[0m 479 \n",
      "Q 898+248 T 1146 \u001b[92m☑\u001b[0m 1146\n",
      "Q 791+7   T 798  \u001b[91m☒\u001b[0m 799 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 329us/step - loss: 0.3351 - acc: 0.9070 - val_loss: 0.3291 - val_acc: 0.9019\n",
      "Q 745+93  T 838  \u001b[92m☑\u001b[0m 838 \n",
      "Q 118+875 T 993  \u001b[91m☒\u001b[0m 192 \n",
      "Q 48+181  T 229  \u001b[91m☒\u001b[0m 239 \n",
      "Q 715+27  T 742  \u001b[92m☑\u001b[0m 742 \n",
      "Q 7+685   T 692  \u001b[91m☒\u001b[0m 792 \n",
      "Q 301+564 T 865  \u001b[92m☑\u001b[0m 865 \n",
      "Q 81+427  T 508  \u001b[92m☑\u001b[0m 508 \n",
      "Q 99+377  T 476  \u001b[92m☑\u001b[0m 476 \n",
      "Q 514+164 T 678  \u001b[91m☒\u001b[0m 688 \n",
      "Q 2+596   T 598  \u001b[92m☑\u001b[0m 598 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 341us/step - loss: 0.2486 - acc: 0.9412 - val_loss: 0.2135 - val_acc: 0.9537\n",
      "Q 11+45   T 56   \u001b[92m☑\u001b[0m 56  \n",
      "Q 20+261  T 281  \u001b[91m☒\u001b[0m 282 \n",
      "Q 393+639 T 1032 \u001b[92m☑\u001b[0m 1032\n",
      "Q 525+235 T 760  \u001b[91m☒\u001b[0m 750 \n",
      "Q 19+139  T 158  \u001b[92m☑\u001b[0m 158 \n",
      "Q 215+39  T 254  \u001b[92m☑\u001b[0m 254 \n",
      "Q 603+67  T 670  \u001b[92m☑\u001b[0m 670 \n",
      "Q 80+715  T 795  \u001b[92m☑\u001b[0m 795 \n",
      "Q 83+761  T 844  \u001b[92m☑\u001b[0m 844 \n",
      "Q 81+560  T 641  \u001b[92m☑\u001b[0m 641 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 312us/step - loss: 0.1850 - acc: 0.9627 - val_loss: 0.1675 - val_acc: 0.9641\n",
      "Q 5+896   T 901  \u001b[92m☑\u001b[0m 901 \n",
      "Q 6+958   T 964  \u001b[92m☑\u001b[0m 964 \n",
      "Q 72+26   T 98   \u001b[92m☑\u001b[0m 98  \n",
      "Q 318+47  T 365  \u001b[92m☑\u001b[0m 365 \n",
      "Q 261+72  T 333  \u001b[92m☑\u001b[0m 333 \n",
      "Q 97+171  T 268  \u001b[92m☑\u001b[0m 268 \n",
      "Q 483+41  T 524  \u001b[92m☑\u001b[0m 524 \n",
      "Q 493+0   T 493  \u001b[92m☑\u001b[0m 493 \n",
      "Q 39+688  T 727  \u001b[92m☑\u001b[0m 727 \n",
      "Q 22+851  T 873  \u001b[92m☑\u001b[0m 873 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 17\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 15s 343us/step - loss: 0.1411 - acc: 0.9747 - val_loss: 0.1253 - val_acc: 0.9780\n",
      "Q 358+281 T 639  \u001b[92m☑\u001b[0m 639 \n",
      "Q 768+3   T 771  \u001b[92m☑\u001b[0m 771 \n",
      "Q 970+83  T 1053 \u001b[92m☑\u001b[0m 1053\n",
      "Q 458+47  T 505  \u001b[92m☑\u001b[0m 505 \n",
      "Q 971+15  T 986  \u001b[92m☑\u001b[0m 986 \n",
      "Q 604+8   T 612  \u001b[92m☑\u001b[0m 612 \n",
      "Q 85+426  T 511  \u001b[92m☑\u001b[0m 511 \n",
      "Q 95+699  T 794  \u001b[91m☒\u001b[0m 784 \n",
      "Q 0+28    T 28   \u001b[91m☒\u001b[0m 38  \n",
      "Q 945+2   T 947  \u001b[92m☑\u001b[0m 947 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 18\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 16s 354us/step - loss: 0.1127 - acc: 0.9805 - val_loss: 0.1636 - val_acc: 0.9563\n",
      "Q 698+19  T 717  \u001b[92m☑\u001b[0m 717 \n",
      "Q 35+461  T 496  \u001b[92m☑\u001b[0m 496 \n",
      "Q 739+232 T 971  \u001b[91m☒\u001b[0m 1071\n",
      "Q 831+6   T 837  \u001b[92m☑\u001b[0m 837 \n",
      "Q 315+4   T 319  \u001b[92m☑\u001b[0m 319 \n",
      "Q 808+908 T 1716 \u001b[91m☒\u001b[0m 1717\n",
      "Q 309+96  T 405  \u001b[92m☑\u001b[0m 405 \n",
      "Q 8+623   T 631  \u001b[92m☑\u001b[0m 631 \n",
      "Q 84+972  T 1056 \u001b[92m☑\u001b[0m 1056\n",
      "Q 92+93   T 185  \u001b[92m☑\u001b[0m 185 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 19\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 16s 358us/step - loss: 0.0951 - acc: 0.9832 - val_loss: 0.0825 - val_acc: 0.9862\n",
      "Q 45+855  T 900  \u001b[91m☒\u001b[0m 800 \n",
      "Q 901+37  T 938  \u001b[92m☑\u001b[0m 938 \n",
      "Q 80+174  T 254  \u001b[92m☑\u001b[0m 254 \n",
      "Q 152+462 T 614  \u001b[92m☑\u001b[0m 614 \n",
      "Q 692+26  T 718  \u001b[92m☑\u001b[0m 718 \n",
      "Q 587+507 T 1094 \u001b[92m☑\u001b[0m 1094\n",
      "Q 6+818   T 824  \u001b[92m☑\u001b[0m 824 \n",
      "Q 15+611  T 626  \u001b[92m☑\u001b[0m 626 \n",
      "Q 0+87    T 87   \u001b[92m☑\u001b[0m 87  \n",
      "Q 220+208 T 428  \u001b[92m☑\u001b[0m 428 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 20\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 303us/step - loss: 0.0701 - acc: 0.9899 - val_loss: 0.0689 - val_acc: 0.9886\n",
      "Q 42+127  T 169  \u001b[92m☑\u001b[0m 169 \n",
      "Q 26+49   T 75   \u001b[92m☑\u001b[0m 75  \n",
      "Q 120+133 T 253  \u001b[92m☑\u001b[0m 253 \n",
      "Q 860+154 T 1014 \u001b[92m☑\u001b[0m 1014\n",
      "Q 309+749 T 1058 \u001b[91m☒\u001b[0m 1078\n",
      "Q 328+3   T 331  \u001b[92m☑\u001b[0m 331 \n",
      "Q 52+868  T 920  \u001b[92m☑\u001b[0m 920 \n",
      "Q 303+407 T 710  \u001b[92m☑\u001b[0m 710 \n",
      "Q 161+9   T 170  \u001b[92m☑\u001b[0m 170 \n",
      "Q 37+857  T 894  \u001b[92m☑\u001b[0m 894 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 21\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 301us/step - loss: 0.0584 - acc: 0.9919 - val_loss: 0.0645 - val_acc: 0.9875\n",
      "Q 563+555 T 1118 \u001b[92m☑\u001b[0m 1118\n",
      "Q 840+8   T 848  \u001b[92m☑\u001b[0m 848 \n",
      "Q 697+109 T 806  \u001b[92m☑\u001b[0m 806 \n",
      "Q 736+91  T 827  \u001b[92m☑\u001b[0m 827 \n",
      "Q 779+78  T 857  \u001b[92m☑\u001b[0m 857 \n",
      "Q 30+680  T 710  \u001b[92m☑\u001b[0m 710 \n",
      "Q 38+942  T 980  \u001b[92m☑\u001b[0m 980 \n",
      "Q 77+70   T 147  \u001b[92m☑\u001b[0m 147 \n",
      "Q 883+92  T 975  \u001b[92m☑\u001b[0m 975 \n",
      "Q 159+96  T 255  \u001b[92m☑\u001b[0m 255 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 22\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 308us/step - loss: 0.0683 - acc: 0.9853 - val_loss: 0.0738 - val_acc: 0.9815\n",
      "Q 68+1    T 69   \u001b[92m☑\u001b[0m 69  \n",
      "Q 785+23  T 808  \u001b[92m☑\u001b[0m 808 \n",
      "Q 878+704 T 1582 \u001b[92m☑\u001b[0m 1582\n",
      "Q 723+2   T 725  \u001b[92m☑\u001b[0m 725 \n",
      "Q 498+0   T 498  \u001b[92m☑\u001b[0m 498 \n",
      "Q 170+508 T 678  \u001b[92m☑\u001b[0m 678 \n",
      "Q 489+27  T 516  \u001b[92m☑\u001b[0m 516 \n",
      "Q 5+246   T 251  \u001b[92m☑\u001b[0m 251 \n",
      "Q 30+635  T 665  \u001b[92m☑\u001b[0m 665 \n",
      "Q 691+604 T 1295 \u001b[92m☑\u001b[0m 1295\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 23\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 303us/step - loss: 0.0413 - acc: 0.9944 - val_loss: 0.0457 - val_acc: 0.9924\n",
      "Q 568+92  T 660  \u001b[92m☑\u001b[0m 660 \n",
      "Q 6+967   T 973  \u001b[92m☑\u001b[0m 973 \n",
      "Q 208+833 T 1041 \u001b[92m☑\u001b[0m 1041\n",
      "Q 117+40  T 157  \u001b[92m☑\u001b[0m 157 \n",
      "Q 152+51  T 203  \u001b[92m☑\u001b[0m 203 \n",
      "Q 686+8   T 694  \u001b[92m☑\u001b[0m 694 \n",
      "Q 131+98  T 229  \u001b[92m☑\u001b[0m 229 \n",
      "Q 371+57  T 428  \u001b[92m☑\u001b[0m 428 \n",
      "Q 181+535 T 716  \u001b[92m☑\u001b[0m 716 \n",
      "Q 24+749  T 773  \u001b[92m☑\u001b[0m 773 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 24\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 304us/step - loss: 0.0325 - acc: 0.9964 - val_loss: 0.0389 - val_acc: 0.9920\n",
      "Q 0+850   T 850  \u001b[92m☑\u001b[0m 850 \n",
      "Q 815+88  T 903  \u001b[92m☑\u001b[0m 903 \n",
      "Q 47+902  T 949  \u001b[91m☒\u001b[0m 959 \n",
      "Q 438+5   T 443  \u001b[92m☑\u001b[0m 443 \n",
      "Q 99+80   T 179  \u001b[92m☑\u001b[0m 179 \n",
      "Q 100+74  T 174  \u001b[92m☑\u001b[0m 174 \n",
      "Q 970+37  T 1007 \u001b[92m☑\u001b[0m 1007\n",
      "Q 13+313  T 326  \u001b[92m☑\u001b[0m 326 \n",
      "Q 26+741  T 767  \u001b[92m☑\u001b[0m 767 \n",
      "Q 246+73  T 319  \u001b[92m☑\u001b[0m 319 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 25\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 304us/step - loss: 0.0477 - acc: 0.9893 - val_loss: 0.0467 - val_acc: 0.9887\n",
      "Q 87+546  T 633  \u001b[92m☑\u001b[0m 633 \n",
      "Q 984+104 T 1088 \u001b[92m☑\u001b[0m 1088\n",
      "Q 112+7   T 119  \u001b[92m☑\u001b[0m 119 \n",
      "Q 6+498   T 504  \u001b[92m☑\u001b[0m 504 \n",
      "Q 3+431   T 434  \u001b[92m☑\u001b[0m 434 \n",
      "Q 514+88  T 602  \u001b[92m☑\u001b[0m 602 \n",
      "Q 510+865 T 1375 \u001b[92m☑\u001b[0m 1375\n",
      "Q 325+82  T 407  \u001b[91m☒\u001b[0m 307 \n",
      "Q 886+45  T 931  \u001b[92m☑\u001b[0m 931 \n",
      "Q 80+60   T 140  \u001b[92m☑\u001b[0m 140 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 26\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 14s 308us/step - loss: 0.0246 - acc: 0.9975 - val_loss: 0.0339 - val_acc: 0.9929\n",
      "Q 889+86  T 975  \u001b[92m☑\u001b[0m 975 \n",
      "Q 641+88  T 729  \u001b[92m☑\u001b[0m 729 \n",
      "Q 57+445  T 502  \u001b[92m☑\u001b[0m 502 \n",
      "Q 15+29   T 44   \u001b[92m☑\u001b[0m 44  \n",
      "Q 738+12  T 750  \u001b[92m☑\u001b[0m 750 \n",
      "Q 86+655  T 741  \u001b[92m☑\u001b[0m 741 \n",
      "Q 176+92  T 268  \u001b[92m☑\u001b[0m 268 \n",
      "Q 575+904 T 1479 \u001b[92m☑\u001b[0m 1479\n",
      "Q 774+20  T 794  \u001b[92m☑\u001b[0m 794 \n",
      "Q 510+865 T 1375 \u001b[92m☑\u001b[0m 1375\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 27\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 17s 385us/step - loss: 0.0223 - acc: 0.9973 - val_loss: 0.0274 - val_acc: 0.9952\n",
      "Q 1+100   T 101  \u001b[92m☑\u001b[0m 101 \n",
      "Q 91+48   T 139  \u001b[92m☑\u001b[0m 139 \n",
      "Q 738+12  T 750  \u001b[92m☑\u001b[0m 750 \n",
      "Q 993+886 T 1879 \u001b[92m☑\u001b[0m 1879\n",
      "Q 881+7   T 888  \u001b[92m☑\u001b[0m 888 \n",
      "Q 943+516 T 1459 \u001b[92m☑\u001b[0m 1459\n",
      "Q 891+470 T 1361 \u001b[92m☑\u001b[0m 1361\n",
      "Q 402+31  T 433  \u001b[92m☑\u001b[0m 433 \n",
      "Q 107+56  T 163  \u001b[92m☑\u001b[0m 163 \n",
      "Q 460+59  T 519  \u001b[92m☑\u001b[0m 519 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 28\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 17s 383us/step - loss: 0.0382 - acc: 0.9911 - val_loss: 0.0228 - val_acc: 0.9961\n",
      "Q 15+69   T 84   \u001b[92m☑\u001b[0m 84  \n",
      "Q 2+596   T 598  \u001b[92m☑\u001b[0m 598 \n",
      "Q 541+532 T 1073 \u001b[92m☑\u001b[0m 1073\n",
      "Q 90+73   T 163  \u001b[92m☑\u001b[0m 163 \n",
      "Q 527+20  T 547  \u001b[92m☑\u001b[0m 547 \n",
      "Q 406+487 T 893  \u001b[92m☑\u001b[0m 893 \n",
      "Q 83+514  T 597  \u001b[92m☑\u001b[0m 597 \n",
      "Q 806+98  T 904  \u001b[92m☑\u001b[0m 904 \n",
      "Q 203+53  T 256  \u001b[92m☑\u001b[0m 256 \n",
      "Q 4+68    T 72   \u001b[92m☑\u001b[0m 72  \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 29\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 18s 389us/step - loss: 0.0158 - acc: 0.9985 - val_loss: 0.0174 - val_acc: 0.9975\n",
      "Q 308+943 T 1251 \u001b[92m☑\u001b[0m 1251\n",
      "Q 5+677   T 682  \u001b[92m☑\u001b[0m 682 \n",
      "Q 15+272  T 287  \u001b[92m☑\u001b[0m 287 \n",
      "Q 196+5   T 201  \u001b[92m☑\u001b[0m 201 \n",
      "Q 0+805   T 805  \u001b[92m☑\u001b[0m 805 \n",
      "Q 1+318   T 319  \u001b[92m☑\u001b[0m 319 \n",
      "Q 935+654 T 1589 \u001b[92m☑\u001b[0m 1589\n",
      "Q 2+643   T 645  \u001b[92m☑\u001b[0m 645 \n",
      "Q 932+90  T 1022 \u001b[92m☑\u001b[0m 1022\n",
      "Q 404+151 T 555  \u001b[92m☑\u001b[0m 555 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 30\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "26112/45000 [================>.............] - ETA: 7s - loss: 0.0128 - acc: 0.9990"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-655bbe31384b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m               validation_data=(x_val, y_val))\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Select 10 samples from the validation set at random so we can visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;31m# errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''An implementation of sequence to sequence learning for performing addition\n",
    "\n",
    "Input: \"535+61\"\n",
    "Output: \"596\"\n",
    "Padding is handled by using a repeated sentinel character (space)\n",
    "\n",
    "Input may optionally be inverted, shown to increase performance in many tasks in:\n",
    "\"Learning to Execute\"\n",
    "http://arxiv.org/abs/1410.4615\n",
    "and\n",
    "\"Sequence to Sequence Learning with Neural Networks\"\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "Theoretically it introduces shorter term dependencies between source and target.\n",
    "\n",
    "Two digits inverted:\n",
    "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
    "\n",
    "Three digits inverted:\n",
    "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
    "\n",
    "Four digits inverted:\n",
    "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
    "\n",
    "Five digits inverted:\n",
    "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "\n",
    "\n",
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One hot encode given string C.\n",
    "\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'\n",
    "\n",
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "INVERT = True\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if INVERT:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "# Try replacing GRU, or SimpleRNN.\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "# As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars))))\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "for iteration in range(1, 200):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=1,\n",
    "              validation_data=(x_val, y_val))\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q[::-1] if INVERT else q, end=' ')\n",
    "        print('T', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + '☒' + colors.close, end=' ')\n",
    "        print(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False],\n",
       "       [ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False],\n",
       "       [ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False],\n",
       "       [False, False, False, False, False,  True, False, False, False,\n",
       "        False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "         True, False, False],\n",
       "       [False,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False],\n",
       "       [False, False, False, False,  True, False, False, False, False,\n",
       "        False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Case: canonical sequence-to-sequence\n",
    "\n",
    "simple machine translation\n",
    "\n",
    "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "\n",
    "https://github.com/fchollet/keras/blob/master/examples/lstm_seq2seq.py\n",
    "\n",
    "\n",
    "## References\n",
    "- Sequence to Sequence Learning with Neural Networks\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "- Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    https://arxiv.org/abs/1406.1078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "latent_dim = 256\n",
    "num_samples = 10000\n",
    "\n",
    "data_path = 'fra-eng/fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "lines = open(data_path).read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)]\n",
    ")\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)]\n",
    ")\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "i 0\n",
    "encode 0 G\n",
    "encode 1 o\n",
    "encode 2 .\n",
    "decode input 0 \t\n",
    "decode input 1 V\n",
    "decode target 1 V\n",
    "decode input 2 a\n",
    "decode target 2 a\n",
    "decode input 3  \n",
    "decode target 3  \n",
    "decode input 4 !\n",
    "decode target 4 !\n",
    "decode input 5 \n",
    "\n",
    "decode target 5 \n",
    "\n",
    "i 1\n",
    "encode 0 R\n",
    "encode 1 u\n",
    "encode 2 n\n",
    "encode 3 !\n",
    "decode input 0 \t\n",
    "decode input 1 C\n",
    "decode target 1 C\n",
    "decode input 2 o\n",
    "decode target 2 o\n",
    "decode input 3 u\n",
    "decode target 3 u\n",
    "decode input 4 r\n",
    "decode target 4 r\n",
    "decode input 5 s\n",
    "decode target 5 s\n",
    "decode input 6  \n",
    "decode target 6  \n",
    "decode input 7 !\n",
    "decode target 7 !\n",
    "decode input 8 \n",
    "\n",
    "decode target 8 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "#     print('i', i)\n",
    "    for t, char in enumerate(input_text):\n",
    "#         print('encode', t, char)\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "#         print('decode input', t, char)\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        # decoder_target_data ahead of decoder_input by 1 step\n",
    "        if t > 0:\n",
    "#             print('decode target', t, char)\n",
    "            decoder_target_data[i, t-1, target_token_index[char]] = 1\n",
    "            \n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                    initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text:  10000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "encoder_input_data (10000, 16, 71)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "decoder_input_data (10000, 59, 93)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "decoder_target_data (10000, 59, 93)\n"
     ]
    }
   ],
   "source": [
    "print('input text: ',len(input_texts))\n",
    "print(encoder_input_data[0])\n",
    "print('encoder_input_data', encoder_input_data.shape)\n",
    "\n",
    "print(decoder_input_data[0])\n",
    "print('decoder_input_data', decoder_input_data.shape)\n",
    "\n",
    "print(decoder_target_data[0])\n",
    "print('decoder_target_data', decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 58s 7ms/step - loss: 0.9307 - val_loss: 0.9735\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.7378 - val_loss: 0.8054\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 56s 7ms/step - loss: 0.6252 - val_loss: 0.7103\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 56s 7ms/step - loss: 0.5659 - val_loss: 0.6632\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 59s 7ms/step - loss: 0.5231 - val_loss: 0.6256\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 57s 7ms/step - loss: 0.4900 - val_loss: 0.5978\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 56s 7ms/step - loss: 0.4629 - val_loss: 0.5790\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 57s 7ms/step - loss: 0.4406 - val_loss: 0.5598\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 57s 7ms/step - loss: 0.4205 - val_loss: 0.5438\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 57s 7ms/step - loss: 0.4024 - val_loss: 0.5323\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 57s 7ms/step - loss: 0.3860 - val_loss: 0.5225\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 58s 7ms/step - loss: 0.3711 - val_loss: 0.5116\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 58s 7ms/step - loss: 0.3571 - val_loss: 0.5018\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 58s 7ms/step - loss: 0.3442 - val_loss: 0.4949\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.3321 - val_loss: 0.4916\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.3207 - val_loss: 0.4881\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.3096 - val_loss: 0.4869\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.2995 - val_loss: 0.4807\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.2896 - val_loss: 0.4810\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.2799 - val_loss: 0.4826\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.2713 - val_loss: 0.4812\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.2624 - val_loss: 0.4806\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.2545 - val_loss: 0.4812\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 66s 8ms/step - loss: 0.2467 - val_loss: 0.4810\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 70s 9ms/step - loss: 0.2393 - val_loss: 0.4808\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 72s 9ms/step - loss: 0.2321 - val_loss: 0.4825\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 68s 8ms/step - loss: 0.2251 - val_loss: 0.4837\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.2184 - val_loss: 0.4947\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 67s 8ms/step - loss: 0.2120 - val_loss: 0.5020\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 67s 8ms/step - loss: 0.2060 - val_loss: 0.4934\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.2000 - val_loss: 0.4985\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.1942 - val_loss: 0.5064\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.1888 - val_loss: 0.5052\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.1833 - val_loss: 0.5147\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.1784 - val_loss: 0.5138\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.1738 - val_loss: 0.5159\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.1691 - val_loss: 0.5236\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.1651 - val_loss: 0.5264\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 65s 8ms/step - loss: 0.1605 - val_loss: 0.5355\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.1563 - val_loss: 0.5356\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 65s 8ms/step - loss: 0.1525 - val_loss: 0.5416\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 66s 8ms/step - loss: 0.1487 - val_loss: 0.5516\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 66s 8ms/step - loss: 0.1454 - val_loss: 0.5535\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.1417 - val_loss: 0.5590\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.1383 - val_loss: 0.5636\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.1351 - val_loss: 0.5672\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.1322 - val_loss: 0.5708\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.1292 - val_loss: 0.5759\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.1263 - val_loss: 0.5816\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.1237 - val_loss: 0.5866\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.1209 - val_loss: 0.5905\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.1183 - val_loss: 0.5962\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.1161 - val_loss: 0.6007\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 65s 8ms/step - loss: 0.1134 - val_loss: 0.6016\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.1113 - val_loss: 0.6098\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.1091 - val_loss: 0.6144\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 66s 8ms/step - loss: 0.1068 - val_loss: 0.6216\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.1047 - val_loss: 0.6257\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.1028 - val_loss: 0.6312\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.1009 - val_loss: 0.6306\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.0992 - val_loss: 0.6353\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.0975 - val_loss: 0.6389\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 0.0954 - val_loss: 0.6400\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 66s 8ms/step - loss: 0.0935 - val_loss: 0.6499\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.0922 - val_loss: 0.6551\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 72s 9ms/step - loss: 0.0907 - val_loss: 0.6517\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 68s 9ms/step - loss: 0.0891 - val_loss: 0.6604\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.0873 - val_loss: 0.6654\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 67s 8ms/step - loss: 0.0859 - val_loss: 0.6652\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 81s 10ms/step - loss: 0.0843 - val_loss: 0.6678\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 74s 9ms/step - loss: 0.0830 - val_loss: 0.6790\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 67s 8ms/step - loss: 0.0815 - val_loss: 0.6811\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.0801 - val_loss: 0.6856\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 59s 7ms/step - loss: 0.0792 - val_loss: 0.6786\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.0778 - val_loss: 0.6914\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 66s 8ms/step - loss: 0.0763 - val_loss: 0.6938\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 66s 8ms/step - loss: 0.0754 - val_loss: 0.7020\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 67s 8ms/step - loss: 0.0742 - val_loss: 0.7052\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.0730 - val_loss: 0.7028\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.0720 - val_loss: 0.7109\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.0703 - val_loss: 0.7173\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.0697 - val_loss: 0.7137\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.0685 - val_loss: 0.7240\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.0676 - val_loss: 0.7218\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 66s 8ms/step - loss: 0.0669 - val_loss: 0.7304\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.0659 - val_loss: 0.7291\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.0647 - val_loss: 0.7301\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.0639 - val_loss: 0.7387\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 65s 8ms/step - loss: 0.0628 - val_loss: 0.7387\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.0620 - val_loss: 0.7466\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 63s 8ms/step - loss: 0.0610 - val_loss: 0.7459\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.0605 - val_loss: 0.7433\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.0594 - val_loss: 0.7539\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 71s 9ms/step - loss: 0.0586 - val_loss: 0.7449\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 66s 8ms/step - loss: 0.0576 - val_loss: 0.7572\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 0.0570 - val_loss: 0.7614\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 64s 8ms/step - loss: 0.0561 - val_loss: 0.7628\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.0554 - val_loss: 0.7664\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.0550 - val_loss: 0.7696\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 62s 8ms/step - loss: 0.0540 - val_loss: 0.7750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narukawinjidtrengam/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:2344: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_3/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_3/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "         batch_size=batch_size,\n",
    "         epochs=epochs,\n",
    "         validation_split=0.2)\n",
    "\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference mode (sampling)\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items()\n",
    ")\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Va !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Courez !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Courez !\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Ça aorre Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Au feu !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: À l'aide !\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Saute.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Stop !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Stop !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Stop !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attendez !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attendez !\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je les vois.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: J'essaye.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: J'ai gagné !\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: J'ai gagné !\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Oh non !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaque !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaque !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Tchin-tchin !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Tchin-tchin !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Tchin-tchin !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Tchin-tchin !\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Lève-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Compris de nous !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Compris de nous !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Pigé ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Pigé ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Pigé ?\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Montez.\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Montez.\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serrez-moi dans vos bras !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serrez-moi dans vos bras !\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis tombée.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je suis tombée.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Je le sais.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je suis partie.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je suis partie.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: J'ai perdu.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: Je suis chanceux.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis voyant.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis voyant.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Écoutez !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vrai ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vrai ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Vrai ?\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Merci !\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: On essaye.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons gagné.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons gagné.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons gagné.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons gagné.\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Demande à Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: Fantastique !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calmes !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calmes !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calmes !\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Sois détendu !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez juste !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez juste !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez juste !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez juste !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez juste !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez juste !\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: Sois gentil.\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sois gentil !\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Dégage !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appellez-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appellez-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Appelle-nous !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Appelle-nous !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Entre !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Entre !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Entre !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Entre !\n",
      "\n",
      "-\n",
      "Input sentence: Come on!\n",
      "Decoded sentence: Aidez !\n",
      "\n",
      "-\n",
      "Input sentence: Come on.\n",
      "Decoded sentence: Viens !\n",
      "\n",
      "-\n",
      "Input sentence: Come on.\n",
      "Decoded sentence: Viens !\n",
      "\n",
      "-\n",
      "Input sentence: Come on.\n",
      "Decoded sentence: Viens !\n",
      "\n",
      "-\n",
      "Input sentence: Drop it!\n",
      "Decoded sentence: Laissez-le tomber !\n",
      "\n",
      "-\n",
      "Input sentence: Drop it!\n",
      "Decoded sentence: Laissez-le tomber !\n",
      "\n",
      "-\n",
      "Input sentence: Drop it!\n",
      "Decoded sentence: Laissez-le tomber !\n",
      "\n",
      "-\n",
      "Input sentence: Drop it!\n",
      "Decoded sentence: Laissez-le tomber !\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: Sortez !\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: Sortez !\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: Sortez !\n",
      "\n",
      "-\n",
      "Input sentence: Get out.\n",
      "Decoded sentence: Sors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decoder_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
