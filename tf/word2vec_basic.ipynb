{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Tutorial \n",
    "https://www.tensorflow.org/tutorials/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narukawinjidtrengam/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://mattmahoney.net/dc/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(filename, expected_bytes):\n",
    "    local_filename = os.path.join(gettempdir(), filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "        local_filename, _ = urllib.request.urlretrieve(url + filename, local_filename)\n",
    "    statinfo = os.stat(local_filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify '+local_filename+'. Can you get to it with a browser?')\n",
    "        \n",
    "    return local_filename\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "filename = 'text8.zip'\n",
    "vocabulary = read_data(filename)\n",
    "\n",
    "print('Data size', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words -1))\n",
    "    dictionary = dict()\n",
    "    for word,_ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0: # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data - list of codes(words are replaced by their codes)\n",
    "# count - map of words(strings) to count of occurrences\n",
    "# dictionary - map of words(strings) to theirs codes(integers)\n",
    "# reversed_dictionary - maps codes(integers) to words(strings)\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n"
     ]
    }
   ],
   "source": [
    "print('Most common words (+UNK)', count[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size,1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [skip_window target skip_window]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size//num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer[:] = data[:span]\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # Backtrack a little a bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    \n",
    "    return batch, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3081 originated -> 5234 anarchism\n",
      "3081 originated -> 12 as\n",
      "12 as -> 3081 originated\n",
      "12 as -> 6 a\n",
      "6 a -> 195 term\n",
      "6 a -> 12 as\n",
      "195 term -> 2 of\n",
      "195 term -> 6 a\n"
     ]
    }
   ],
   "source": [
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]],\n",
    "         '->', labels[i, 0], reverse_dictionary[labels[i,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "skip_window = 1\n",
    "num_skips = 2\n",
    "num_sampled = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 16\n",
    "valid_window = 100\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        embeddings = tf.Variable(tf.random_uniform(\n",
    "            [vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        \n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                               stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Compute the average NCE loss for the batch.\n",
    "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "  # time we evaluate the loss.\n",
    "  # Explanation of the meaning of NCE loss:\n",
    "  #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                      biases=nce_biases,\n",
    "                      labels=train_labels,\n",
    "                      inputs=embed,\n",
    "                      num_sampled=num_sampled,\n",
    "                      num_classes=vocabulary_size)\n",
    "        )\n",
    "    \n",
    "    # SGD optimizer using learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "    \n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings/norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    \n",
    "    # Add variable initializer\n",
    "    init = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized.\n",
      "Average loss at step  0 :  286.504394531\n",
      "Nearest to has: malaria bethlen cistercian giacomo omer rite gyeonggi guayaquil\n",
      "Nearest to seven: bonuses structure toss lenten tsarevich merchandise snowmobile andr\n",
      "Nearest to may: flywheels kaf prospectors michaux avogadro deflate loft eoin\n",
      "Nearest to on: assay afb filial machined exquisite humus newspaper dumont\n",
      "Nearest to four: collapses suitor pavlov chimneys venerable uniqueness schirra jacobitism\n",
      "Nearest to his: writable tendencies saloon irons plot damien lena tombstone\n",
      "Nearest to more: uss email purse hobbit automation outpost horn whitney\n",
      "Nearest to this: kyle bouvines descriptions poem dalit bipedalism lapis sandhi\n",
      "Nearest to who: underdogs diverges gepids stiff dixon transistors honoring answer\n",
      "Nearest to d: khalil align webmineral feat despotism personification browsing fu\n",
      "Nearest to some: pharmacy coriander ben undoing distributist hoyle storks stanislavski\n",
      "Nearest to there: mena scrimmage comparably sorcerers celesta bilingualism lasker mestizos\n",
      "Nearest to were: dmitri flutist ism methane wisconsin motive styne document\n",
      "Nearest to world: indoctrination attendance fatigue affinity diffuses deluxe lengthened bahama\n",
      "Nearest to known: keel alter danforth erewhon arminius calligraphy early byu\n",
      "Nearest to from: rationalisation adorno dupuis substantive tasting intrigues grievance bullshit\n",
      "Average loss at step  2000 :  114.41032351\n",
      "Average loss at step  4000 :  52.8784305053\n",
      "Average loss at step  6000 :  33.297719274\n",
      "Average loss at step  8000 :  23.3215707687\n",
      "Average loss at step  10000 :  17.3073718929\n",
      "Nearest to has: is have train stravinsky philosophers rite circ ronstadt\n",
      "Nearest to seven: zero nine township six netbios nf cc five\n",
      "Nearest to may: nine latitude giving mtu loaded fuel demonstrate translated\n",
      "Nearest to on: in for of and cc one two is\n",
      "Nearest to four: nine zero township cc eurocents two five three\n",
      "Nearest to his: the president plot netbios decibel a profile some\n",
      "Nearest to more: uss skill fletcher writers hobbit email eurocents sterile\n",
      "Nearest to this: nf which the hz descriptions eurocents netbios reach\n",
      "Nearest to who: entities dor stiff dundee mercenary and wanderer what\n",
      "Nearest to d: township ages align and emmylou launched netbios remain\n",
      "Nearest to some: ben datura distributist pharmacy township nf decibel his\n",
      "Nearest to there: netbios township taught eurocents plural blindness ronstadt extremely\n",
      "Nearest to were: was dmitri is driver homeomorphic document sterile tons\n",
      "Nearest to world: royals approx attendance if fatigue township situations diffuses\n",
      "Nearest to known: early alter variation braque dor township dkw suffering\n",
      "Nearest to from: of in and netbios for with inductive mtu\n",
      "Average loss at step  12000 :  14.1049002646\n",
      "Average loss at step  14000 :  11.4909787775\n",
      "Average loss at step  16000 :  9.4133582114\n",
      "Average loss at step  18000 :  8.7231368798\n",
      "Average loss at step  20000 :  8.01276849103\n",
      "Nearest to has: is have was had train in stravinsky philosophers\n",
      "Nearest to seven: zero nine six eight five township four three\n",
      "Nearest to may: giving and nine to goku latitude baden loaded\n",
      "Nearest to on: in for and two mtu with cc at\n",
      "Nearest to four: three five zero eight nine seven township six\n",
      "Nearest to his: the their some her president netbios a tendencies\n",
      "Nearest to more: khanate skill uss automation excavations whitney fletcher email\n",
      "Nearest to this: the which nf it crescent a one netbios\n",
      "Nearest to who: entities and which stiff dor mercenary dixon he\n",
      "Nearest to d: UNK township ages s align launched odds pope\n",
      "Nearest to some: many the his ben their datura contracts nf\n",
      "Nearest to there: which taught not extremely workings township this blindness\n",
      "Nearest to were: are was is driver dmitri petrels as by\n",
      "Nearest to world: attendance royals approx industry dravidian evolutionism wood bearer\n",
      "Nearest to known: a alter early braque dkw dor variation erewhon\n",
      "Nearest to from: in and of for with daleks netbios after\n",
      "Average loss at step  22000 :  7.44109239864\n",
      "Average loss at step  24000 :  6.99521870041\n",
      "Average loss at step  26000 :  6.60640685737\n",
      "Average loss at step  28000 :  6.34953924561\n",
      "Average loss at step  30000 :  5.94431077909\n",
      "Nearest to has: had is have was train stravinsky discusses by\n",
      "Nearest to seven: eight nine five six four zero three township\n",
      "Nearest to may: can must will to phoca goku dxm nine\n",
      "Nearest to on: in at and for mtu two wootz palatalization\n",
      "Nearest to four: five three eight six seven two zero nine\n",
      "Nearest to his: her the their kiang some netbios a president\n",
      "Nearest to more: khanate hobbit brie uss skill writers excavations tailed\n",
      "Nearest to this: the which it nf brie crescent a phoca\n",
      "Nearest to who: which he entities and stiff mercenary dixon they\n",
      "Nearest to d: UNK township econometric ages o pope allegheny odds\n",
      "Nearest to some: many the their his datura modulated ben nf\n",
      "Nearest to there: which it brie not this they leyte workings\n",
      "Nearest to were: are was is by do driver had been\n",
      "Nearest to world: attendance royals approx industry dravidian evolutionism affinity emulsions\n",
      "Nearest to known: alter a early braque dkw variation dor erewhon\n",
      "Nearest to from: in for with and at nine between after\n",
      "Average loss at step  32000 :  5.79625253963\n",
      "Average loss at step  34000 :  5.70686331677\n",
      "Average loss at step  36000 :  5.70284157014\n",
      "Average loss at step  38000 :  5.48871118426\n",
      "Average loss at step  40000 :  5.35254299617\n",
      "Nearest to has: had have is was stravinsky discusses by train\n",
      "Nearest to seven: eight six five nine four three zero two\n",
      "Nearest to may: can must would will flywheels should to could\n",
      "Nearest to on: in at mtu palatalization wootz for two seven\n",
      "Nearest to four: six five three eight seven two nine zero\n",
      "Nearest to his: their her the kiang its some s netbios\n",
      "Nearest to more: less khanate excavations brie writers mode adventurer automation\n",
      "Nearest to this: which it the nf brie crescent kiang eurocents\n",
      "Nearest to who: he which they and entities stiff mercenary fln\n",
      "Nearest to d: b fenrir township econometric pope UNK novello recordings\n",
      "Nearest to some: many their the other modulated his datura each\n",
      "Nearest to there: it which they brie this not often he\n",
      "Nearest to were: are was is do had by been have\n",
      "Nearest to world: attendance royals affinity fenrir approx falsified industry emulsions\n",
      "Nearest to known: a early alter filed braque dor quite variation\n",
      "Nearest to from: in for between at with and of after\n",
      "Average loss at step  42000 :  5.71513828135\n",
      "Average loss at step  44000 :  5.27598827684\n",
      "Average loss at step  46000 :  5.21825450742\n",
      "Average loss at step  48000 :  5.17304317486\n",
      "Average loss at step  50000 :  5.06152839935\n",
      "Nearest to has: had have is was stravinsky cc discusses eugenic\n",
      "Nearest to seven: eight six five four nine three zero two\n",
      "Nearest to may: can would must will should could might flywheels\n",
      "Nearest to on: in at two for during mtu and wootz\n",
      "Nearest to four: three five six eight seven two zero nine\n",
      "Nearest to his: their her the its kiang some s netbios\n",
      "Nearest to more: less writers adventurer khanate mode excavations automation tailed\n",
      "Nearest to this: which it the nf that brie crescent modula\n",
      "Nearest to who: he which they entities there fln mercenary also\n",
      "Nearest to d: b econometric fenrir township o pope novello stadtbahn\n",
      "Nearest to some: many their other his the modulated all each\n",
      "Nearest to there: it they which brie he often modula not\n",
      "Nearest to were: are was is had have be do been\n",
      "Nearest to world: attendance royals modula snobol fenrir dravidian affinity falsified\n",
      "Nearest to known: szko a modula early used alter filed braque\n",
      "Nearest to from: in at with and after between for four\n",
      "Average loss at step  52000 :  4.95494669867\n",
      "Average loss at step  54000 :  5.09098194814\n",
      "Average loss at step  56000 :  5.12991239524\n",
      "Average loss at step  58000 :  5.07287809348\n",
      "Average loss at step  60000 :  5.03477398789\n",
      "Nearest to has: had have was is cc stravinsky eugenic dsn\n",
      "Nearest to seven: eight six five four nine three zero one\n",
      "Nearest to may: can would must will could should might nine\n",
      "Nearest to on: in at during mtu two from palatalization parakeet\n",
      "Nearest to four: five six three eight seven two zero one\n",
      "Nearest to his: their her its the s kiang some netbios\n",
      "Nearest to more: less most really adventurer mode writers automation khanate\n",
      "Nearest to this: which it the brie nf that crescent sza\n",
      "Nearest to who: he and which they entities there conforming fln\n",
      "Nearest to d: b UNK o six pope novello township fenrir\n",
      "Nearest to some: many their the all other each these modulated\n",
      "Nearest to there: it they which he brie often usually modula\n",
      "Nearest to were: are was have had is been do be\n",
      "Nearest to world: attendance royals modula industry affinity snobol fenrir dravidian\n",
      "Nearest to known: astm szko used a modula filed early alter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to from: in at between after into and on eight\n",
      "Average loss at step  62000 :  4.96239105809\n",
      "Average loss at step  64000 :  4.88086432087\n",
      "Average loss at step  66000 :  4.78974966991\n",
      "Average loss at step  68000 :  4.82754983079\n",
      "Average loss at step  70000 :  4.86707739472\n",
      "Nearest to has: had have was is eugenic cc became dsn\n",
      "Nearest to seven: six eight five four three nine zero two\n",
      "Nearest to may: can would must will could should might cannot\n",
      "Nearest to on: in at during wootz mtu parakeet striping glas\n",
      "Nearest to four: six five seven three eight two nine one\n",
      "Nearest to his: their her its the kiang s some modula\n",
      "Nearest to more: less most adventurer mode brie writers really desire\n",
      "Nearest to this: which it the that brie nf one crescent\n",
      "Nearest to who: he which they and there conforming entities it\n",
      "Nearest to d: b khalil o novello pope UNK township fenrir\n",
      "Nearest to some: many their all these other several each certain\n",
      "Nearest to there: it they he which brie usually often modula\n",
      "Nearest to were: are was have had be is do been\n",
      "Nearest to world: attendance royals modula snobol industry fenrir affinity dravidian\n",
      "Nearest to known: used a astm szko modula filed quite braque\n",
      "Nearest to from: in into at between and by after with\n",
      "Average loss at step  72000 :  4.82950960958\n",
      "Average loss at step  74000 :  4.82435538757\n",
      "Average loss at step  76000 :  4.83959668899\n",
      "Average loss at step  78000 :  4.76008965099\n",
      "Average loss at step  80000 :  4.79135369384\n",
      "Nearest to has: had have was is hyi became cc eugenic\n",
      "Nearest to seven: six eight five nine four three zero two\n",
      "Nearest to may: can would will must could should might hyi\n",
      "Nearest to on: in at during wootz palatalization mtu two creeping\n",
      "Nearest to four: five six seven three eight two nine zero\n",
      "Nearest to his: their her its the s kiang landesverband landsmannschaft\n",
      "Nearest to more: less most adventurer mode really tailed writers brie\n",
      "Nearest to this: which it the that brie landesverband nf another\n",
      "Nearest to who: he they which conforming there and entities intergovernmental\n",
      "Nearest to d: b UNK khalil novello o township pope stadtbahn\n",
      "Nearest to some: many all their these several the other certain\n",
      "Nearest to there: they it he which often landesverband usually brie\n",
      "Nearest to were: are was have had be been do is\n",
      "Nearest to world: royals modula attendance mvs snobol fenrir affinity industry\n",
      "Nearest to known: used astm szko a filed such modula early\n",
      "Nearest to from: in into after between within at and nine\n",
      "Average loss at step  82000 :  4.71238730872\n",
      "Average loss at step  84000 :  4.76986985147\n",
      "Average loss at step  86000 :  4.54509742677\n",
      "Average loss at step  88000 :  4.65100256753\n",
      "Average loss at step  90000 :  4.71439863038\n",
      "Nearest to has: had have was is became hyi cc eugenic\n",
      "Nearest to seven: six five four eight nine three zero two\n",
      "Nearest to may: can would will could must should might cannot\n",
      "Nearest to on: in at during palatalization striping wootz and mtu\n",
      "Nearest to four: five six three seven eight two zero nine\n",
      "Nearest to his: their her its the kiang s landesverband landsmannschaft\n",
      "Nearest to more: less most adventurer really writers very tailed brie\n",
      "Nearest to this: which it the nf another brie that landesverband\n",
      "Nearest to who: he they which conforming there often never intergovernmental\n",
      "Nearest to d: b UNK o khalil novello pope township stadtbahn\n",
      "Nearest to some: many all these several their other the certain\n",
      "Nearest to there: they it he usually she which often landesverband\n",
      "Nearest to were: are was have had be been do datura\n",
      "Nearest to world: royals fenrir affinity attendance mvs modula snobol industry\n",
      "Nearest to known: used astm such szko filed a modula braque\n",
      "Nearest to from: in into at within and after between nine\n",
      "Average loss at step  92000 :  4.64997600091\n",
      "Average loss at step  94000 :  4.63161966681\n",
      "Average loss at step  96000 :  4.60875581384\n",
      "Average loss at step  98000 :  4.61513939166\n",
      "Average loss at step  100000 :  4.37259804559\n",
      "Nearest to has: had have is was became eugenic hyi seven\n",
      "Nearest to seven: five six eight four three nine zero two\n",
      "Nearest to may: can would will could must should might eight\n",
      "Nearest to on: in at during creeping striping wootz palatalization mtu\n",
      "Nearest to four: five seven three six eight nine zero two\n",
      "Nearest to his: their her its the kiang s landesverband hyi\n",
      "Nearest to more: less most adventurer mode very really writers desire\n",
      "Nearest to this: which it the nf brie another landesverband that\n",
      "Nearest to who: he and they which conforming never there often\n",
      "Nearest to d: b and khalil recordings novello pope o fenrir\n",
      "Nearest to some: many all these several their certain other each\n",
      "Nearest to there: they it he usually often she which brie\n",
      "Nearest to were: are was have had is be been do\n",
      "Nearest to world: modula fenrir mvs royals dravidian affinity landesverband industry\n",
      "Nearest to known: used such astm filed szko regarded modula sympathy\n",
      "Nearest to from: in into within after at between through during\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    print('initialized.')\n",
    "    \n",
    "    average_loss = 0\n",
    "    \n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "        \n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "        \n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s' % (log_str, close_word)\n",
    "                    \n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs, labels, filename):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    \n",
    "    plt.figure(figsize=(18,18))\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                    xy=(x,y),\n",
    "                    xytext=(5,2),\n",
    "                    textcoords='offset points',\n",
    "                    ha='right',\n",
    "                    va='bottom')\n",
    "        \n",
    "        plt.savefig(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "    plot_only= 500\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "    plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(), 'tsne.png'))\n",
    "    \n",
    "except ImportError as ex:\n",
    "    print('Please install sklearn, matplotlib, and scipy')\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
